# Decision Logic V2.0 Extension
# ACS-Mentor V2.0 æ–°å¢å†³ç­–é€»è¾‘

**åŸºç¡€**: æœ¬æ–‡æ¡£æ‰©å±•`decision_logic_guide.md`(V1.2.1)
**ç‰ˆæœ¬**: V2.0
**æ—¥æœŸ**: 2025-11-13

---

## æ¦‚è¿°

V2.0åœ¨V1.2.1çš„6å› å­å†³ç­–æ¡†æ¶åŸºç¡€ä¸Šï¼Œæ–°å¢2ä¸ªå› å­ä»¥æ”¯æŒå¯¼å¸ˆæ¨¡å¼ï¼š

```
V1.2.1 (6 Factors - Critic Mode)        V2.0 (8 Factors - Dual Mode)
â”œâ”€â”€ error_detection                     â”œâ”€â”€ error_detection
â”œâ”€â”€ goal_threatened                     â”œâ”€â”€ goal_threatened
â”œâ”€â”€ expertise_match                     â”œâ”€â”€ expertise_match
â”œâ”€â”€ misrepresented                      â”œâ”€â”€ misrepresented
â”œâ”€â”€ silence_too_long                    â”œâ”€â”€ silence_too_long
â””â”€â”€ agenda_opportunity                  â”œâ”€â”€ agenda_opportunity
                                        â”œâ”€â”€ growth_opportunity â­ NEW
                                        â””â”€â”€ strategic_insight  â­ NEW
```

---

## Factor 7: Growth Opportunity (æˆé•¿æœºä¼š)

### å®šä¹‰

æ£€æµ‹ç”¨æˆ·çš„å­¦ä¹ æ—¶åˆ»å’Œèƒ½åŠ›å‘å±•æœºä¼šã€‚

**ä¸error_detectionçš„åŒºåˆ«**:
- `error_detection`: å·²ç»çŠ¯çš„é”™è¯¯ï¼ˆreactiveï¼‰
- `growth_opportunity`: å¯ä»¥å­¦ä¹ çš„æœºä¼šï¼ˆproactiveï¼‰ï¼Œå³ä½¿æ²¡çŠ¯é”™

### æ£€æµ‹ç®—æ³•

```python
def detect_growth_opportunity(user_message, user_profile, history):
    """
    æ£€æµ‹ç”¨æˆ·æˆé•¿æœºä¼š

    Returns:
        score: 0.0-1.0
        opportunities: list of detected opportunity types
    """
    score = 0.0
    opportunities = []

    # A. ç”¨æˆ·è¡¨è¾¾å›°æƒ‘æˆ–ä¸ç¡®å®š
    uncertainty_signals = [
        "ä¸çŸ¥é“", "ä¸ç¡®å®š", "æ˜¯å¦å¯ä»¥", "å“ªä¸ªæ›´å¥½", "åº”è¯¥ç”¨",
        "confused", "not sure", "which is better", "should I use"
    ]

    if any(sig in user_message.lower() for sig in uncertainty_signals):
        score += 0.8
        opportunities.append("user_expressed_uncertainty")

    # B. ç”¨æˆ·å¤„äºå…³é”®å†³ç­–ç‚¹
    decision_points = {
        "study_design": ["è®¾è®¡", "design", "RCT", "è§‚å¯Ÿæ€§", "cohort"],
        "statistical_method": ["ç»Ÿè®¡", "åˆ†æ", "æ£€éªŒ", "æ¨¡å‹", "å›å½’"],
        "sample_size": ["æ ·æœ¬é‡", "sample size", "power", "åŠŸæ•ˆ"],
        "interpretation": ["è§£é‡Š", "interpret", "æ„ä¹‰", "significance"],
    }

    for decision_type, keywords in decision_points.items():
        if any(kw in user_message for kw in keywords):
            score += 0.6
            opportunities.append(f"decision_point_{decision_type}")
            break  # åªè®¡åˆ†ä¸€æ¬¡

    # C. ç”¨æˆ·å±•ç¤ºå­¦ä¹ æ„æ„¿
    learning_signals = [
        "ä¸ºä»€ä¹ˆ", "æ€ä¹ˆåš", "èƒ½æ•™æˆ‘", "å¦‚ä½•", "åŸç†",
        "why", "how to", "teach me", "explain", "principle"
    ]

    if any(sig in user_message.lower() for sig in learning_signals):
        score += 0.7
        opportunities.append("learning_intent_detected")

    # D. åŸºäºç”¨æˆ·å†å²çš„gapè¯†åˆ«
    if user_profile and hasattr(user_profile, 'skill_gaps'):
        current_topic = extract_topic(user_message)
        if current_topic in user_profile.skill_gaps:
            score += 0.9
            opportunities.append("known_skill_gap")

    # E. Recurring error pattern (from mentorship_goals.yaml)
    if history:
        recent_errors = get_recent_error_patterns(history, window=10)
        for pattern in recent_errors:
            if pattern.count >= 2 and is_relevant_to(pattern, user_message):
                score += 0.95  # é«˜ä¼˜å…ˆçº§
                opportunities.append(f"recurring_pattern_{pattern.id}")
                break

    return min(score, 1.0), opportunities


def extract_topic(user_message):
    """
    ç®€åŒ–ç‰ˆ: æå–æ¶ˆæ¯ä¸»é¢˜
    å®é™…å®ç°å¯ä»¥æ›´å¤æ‚
    """
    # å…³é”®è¯åŒ¹é…
    topics = {
        "sample_size": ["æ ·æœ¬é‡", "sample size", "power"],
        "validation": ["éªŒè¯", "validation", "cross-validation"],
        "causality": ["å› æœ", "causal", "å¯¼è‡´", "cause"],
        # ... æ›´å¤šä¸»é¢˜
    }

    for topic, keywords in topics.items():
        if any(kw in user_message.lower() for kw in keywords):
            return topic

    return "general"


def is_relevant_to(pattern, user_message):
    """æ£€æŸ¥error patternæ˜¯å¦ä¸å½“å‰æ¶ˆæ¯ç›¸å…³"""
    pattern_keywords = {
        "multiple_testing_å¿˜è®°æ ¡æ­£": ["æ£€éªŒ", "æ¯”è¾ƒ", "test", "comparison"],
        "validation_é—æ¼": ["æ¨¡å‹", "é¢„æµ‹", "AUC", "performance"],
        "å› æœè¯­è¨€_observational": ["å¯¼è‡´", "cause", "effect", "å½±å“"],
    }

    if pattern.id in pattern_keywords:
        keywords = pattern_keywords[pattern.id]
        return any(kw in user_message.lower() for kw in keywords)

    return False
```

### ç¤ºä¾‹åœºæ™¯

```python
# åœºæ™¯1: ç”¨æˆ·è¡¨è¾¾å›°æƒ‘
user_message = "æˆ‘ä¸ç¡®å®šåº”è¯¥ç”¨logistic regressionè¿˜æ˜¯log-binomial regression"
score, opps = detect_growth_opportunity(user_message, user_profile, history)
# â†’ score â‰ˆ 0.8 (uncertainty) + 0.6 (decision_point)
# â†’ opps = ["user_expressed_uncertainty", "decision_point_statistical_method"]

# åœºæ™¯2: ç”¨æˆ·ä¸»åŠ¨å­¦ä¹ 
user_message = "ä¸ºä»€ä¹ˆè¦åšvalidationï¼Ÿèƒ½æ•™æˆ‘validationçš„åŸç†å—ï¼Ÿ"
score, opps = detect_growth_opportunity(user_message, user_profile, history)
# â†’ score â‰ˆ 0.7 (learning_intent) + 0.6 (decision_point_validation)

# åœºæ™¯3: Recurring pattern
user_message = "æˆ‘å»ºäº†ä¸€ä¸ªé¢„æµ‹æ¨¡å‹ï¼ŒAUCæ˜¯0.85"
# å‡è®¾historyæ˜¾ç¤ºç”¨æˆ·å·²2æ¬¡é—æ¼validation
score, opps = detect_growth_opportunity(user_message, user_profile, history)
# â†’ score â‰ˆ 0.95 (recurring_pattern)
# â†’ opps = ["recurring_pattern_validation_é—æ¼"]
```

---

## Factor 8: Strategic Insight (æˆ˜ç•¥æ´å¯Ÿ)

### å®šä¹‰

æ£€æµ‹æä¾›é«˜å±‚æ¬¡è§†è§’å’Œæˆ˜ç•¥å»ºè®®çš„æœºä¼šã€‚

**ä¸å…¶ä»–å› å­çš„åŒºåˆ«**:
- ä¸æ˜¯çº é”™ï¼ˆerror_detectionï¼‰
- ä¸æ˜¯æ•™å­¦ï¼ˆgrowth_opportunityï¼‰
- è€Œæ˜¯æä¾›**big picture thinking**

### æ£€æµ‹ç®—æ³•

```python
def detect_strategic_insight_opportunity(user_message, user_profile, context):
    """
    æ£€æµ‹æˆ˜ç•¥æ´å¯Ÿæœºä¼š

    Returns:
        score: 0.0-1.0
        insights: list of detected insight opportunities
    """
    score = 0.0
    insights = []

    # A. ç”¨æˆ·åœ¨è§„åˆ’æˆ–é€‰æ‹©ç ”ç©¶æ–¹å‘
    planning_signals = [
        "æƒ³åš", "è®¡åˆ’", "æ‰“ç®—", "æ˜¯å¦å€¼å¾—", "æœ‰æ²¡æœ‰æ„ä¹‰",
        "planning", "considering", "worth", "should I pursue"
    ]

    if any(sig in user_message.lower() for sig in planning_signals):
        score += 0.8
        insights.append("research_planning")

    # B. ç”¨æˆ·è¯¢é—®å‰æ²¿æˆ–è¶‹åŠ¿
    frontier_signals = [
        "æœ€æ–°", "å‰æ²¿", "è¶‹åŠ¿", "çƒ­ç‚¹", "åˆ›æ–°", "æœªæ¥",
        "latest", "frontier", "trend", "hot topic", "future direction"
    ]

    if any(sig in user_message.lower() for sig in frontier_signals):
        score += 0.85
        insights.append("frontier_inquiry")

    # C. ç”¨æˆ·é¢ä¸´æˆ˜ç•¥é€‰æ‹©
    choice_signals = [
        "é€‰å“ªä¸ª", "è¿˜æ˜¯", "æˆ–è€…", "ä¸¤è€…", "æ¯”è¾ƒ", "vs",
        "which", "or", "versus", "compare", "between"
    ]

    # éœ€è¦æœ‰å®é™…çš„é€‰é¡¹
    if any(sig in user_message.lower() for sig in choice_signals):
        # æ£€æŸ¥æ˜¯å¦çœŸçš„åœ¨æ¯”è¾ƒç ”ç©¶æ–¹å‘ï¼ˆvs. ä»…æ¯”è¾ƒç»Ÿè®¡æ–¹æ³•ï¼‰
        if contains_research_direction(user_message):
            score += 0.75
            insights.append("strategic_choice")

    # D. ç”¨æˆ·æè¿°ç ”ç©¶æƒ³æ³•ï¼ˆä¸»åŠ¨æä¾›åˆ›æ–°æ€§è¯„ä¼°ï¼‰
    if contains_research_idea(user_message):
        score += 0.70
        insights.append("idea_assessment_opportunity")

    # E. ç”¨æˆ·è®¨è®ºé•¿æœŸè§„åˆ’
    longterm_signals = [
        "èŒä¸š", "é•¿æœŸ", "5å¹´", "10å¹´", "ç ”ç©¶çº¿",
        "career", "long-term", "research line", "program"
    ]

    if any(sig in user_message.lower() for sig in longterm_signals):
        score += 0.80
        insights.append("longterm_planning")

    # F. ç”¨æˆ·å¤„äºcareer transition
    if user_profile and user_profile.at_career_transition():
        score += 0.65
        insights.append("career_transition")

    return min(score, 1.0), insights


def contains_research_direction(user_message):
    """æ£€æŸ¥æ˜¯å¦åœ¨è®¨è®ºç ”ç©¶æ–¹å‘"""
    direction_keywords = [
        "ç ”ç©¶æ–¹å‘", "è¯¾é¢˜", "é¡¹ç›®", "æ–¹å‘", "é¢†åŸŸ",
        "research direction", "topic", "project", "field"
    ]
    return any(kw in user_message.lower() for kw in direction_keywords)


def contains_research_idea(user_message):
    """æ£€æŸ¥æ˜¯å¦æè¿°äº†ç ”ç©¶æƒ³æ³•"""
    idea_signals = [
        "æƒ³æ³•", "idea", "hypothesis", "å‡è®¾",
        "æƒ³ç ”ç©¶", "æƒ³æ¢ç´¢", "plan to study"
    ]

    # éœ€è¦æœ‰æƒ³æ³• + å…·ä½“å†…å®¹
    has_idea_signal = any(sig in user_message.lower() for sig in idea_signals)
    has_substance = len(user_message) > 50  # ç®€åŒ–åˆ¤æ–­

    return has_idea_signal and has_substance
```

### ç¤ºä¾‹åœºæ™¯

```python
# åœºæ™¯1: ç ”ç©¶æ–¹å‘è§„åˆ’
user_message = "æˆ‘åœ¨è€ƒè™‘åšAIåœ¨ä¸´åºŠå†³ç­–ä¸­çš„åº”ç”¨ï¼Œä¸çŸ¥é“æ˜¯å¦å€¼å¾—æŠ•å…¥"
score, insights = detect_strategic_insight_opportunity(user_message, user_profile, context)
# â†’ score â‰ˆ 0.8 (planning) + 0.7 (idea_assessment)
# â†’ insights = ["research_planning", "idea_assessment_opportunity"]

# åœºæ™¯2: å‰æ²¿è¶‹åŠ¿å’¨è¯¢
user_message = "ç°åœ¨å› æœæ¨æ–­é¢†åŸŸæœ‰ä»€ä¹ˆå‰æ²¿æ–¹æ³•å—ï¼Ÿ"
score, insights = detect_strategic_insight_opportunity(user_message, user_profile, context)
# â†’ score â‰ˆ 0.85 (frontier_inquiry)

# åœºæ™¯3: æˆ˜ç•¥é€‰æ‹©
user_message = "æˆ‘åœ¨è€ƒè™‘ä¸¤ä¸ªç ”ç©¶æ–¹å‘ï¼šç²¾å‡†åŒ»ç–— vs. å¥åº·å…¬å¹³ï¼Œå“ªä¸ªæ›´æœ‰å‰æ™¯ï¼Ÿ"
score, insights = detect_strategic_insight_opportunity(user_message, user_profile, context)
# â†’ score â‰ˆ 0.75 (strategic_choice)
```

---

## V2.0 Urgencyè®¡ç®—

### 8å› å­ç»Ÿä¸€è®¡ç®—

```python
def calculate_urgency_v2(factors, weights, mode='balanced'):
    """
    V2.0: 8å› å­urgencyè®¡ç®—

    Args:
        factors: dict with 8 factor scores
        weights: dict with base weights (from beliefs.yaml)
        mode: 'critic' | 'mentor' | 'balanced'

    Returns:
        urgency_score: float
    """

    # åº”ç”¨mode-specificæƒé‡è°ƒæ•´
    adjusted_weights = apply_mode_adjustments(weights, mode)

    # è®¡ç®—base urgency (6å› å­ - V1.2.1)
    base_urgency = (
        factors['error_detection'] * adjusted_weights['error_detection'] +
        factors['goal_threatened'] * adjusted_weights['goal_threatened'] +
        factors['expertise_match'] * adjusted_weights['expertise_match'] +
        factors['misrepresented'] * adjusted_weights['misrepresented'] +
        factors['silence_too_long'] * adjusted_weights['silence_too_long'] +
        factors['agenda_opportunity'] * adjusted_weights['agenda_opportunity']
    )

    # è®¡ç®—mentorship urgency (2æ–°å› å­ - V2.0)
    mentorship_urgency = (
        factors['growth_opportunity'] * adjusted_weights['growth_opportunity'] +
        factors['strategic_insight'] * adjusted_weights['strategic_insight']
    )

    # æ€»urgency
    total_urgency = base_urgency + mentorship_urgency

    # ç†è®ºæœ€å¤§å€¼ = 0.9+0.8+0.6+0.7+0.4+0.75+0.7+0.65 = 5.5
    # å®é™…æœ€å¤§å€¼çº¦3.0ï¼ˆå¾ˆå°‘æ‰€æœ‰å› å­éƒ½é«˜åˆ†ï¼‰
    return min(total_urgency, 3.5)


def apply_mode_adjustments(base_weights, mode):
    """
    åº”ç”¨mode-specificæƒé‡è°ƒæ•´

    ä»beliefs.yamlä¸­çš„mode_specific_weightsè¯»å–
    """
    adjusted = base_weights.copy()

    mode_adjustments = {
        'critic': {
            'error_detection': 1.0,
            'goal_threatened': 1.0,
            'agenda_opportunity': 1.0,
            'growth_opportunity': 0.5,
            'strategic_insight': 0.3,
        },
        'mentor': {
            'error_detection': 0.7,
            'goal_threatened': 0.7,
            'growth_opportunity': 1.2,
            'strategic_insight': 1.1,
            'agenda_opportunity': 0.8,
        },
        'balanced': {
            # ä¸è°ƒæ•´ï¼Œä½¿ç”¨base weights
        }
    }

    if mode in mode_adjustments:
        for factor, multiplier in mode_adjustments[mode].items():
            if factor in adjusted:
                adjusted[factor] = base_weights[factor] * multiplier

    return adjusted
```

---

## æ¨¡å¼é€‰æ‹©é€»è¾‘

### æ™ºèƒ½æ¨¡å¼é€‰æ‹©

```python
def select_response_mode(urgency_breakdown, user_profile):
    """
    æ ¹æ®urgency breakdownæ™ºèƒ½é€‰æ‹©å“åº”æ¨¡å¼

    Returns: {
        'mode': 'critic' | 'mentor' | 'hybrid',
        'primary_pattern': 'A' | 'B' | 'C' | 'D' | 'M-A' | 'M-B' | 'M-C',
        'mentorship_layer': None | 'teaching' | 'strategic' | 'celebration'
    }
    """

    # è®¡ç®—å„æ¨¡å¼çš„urgency
    critic_score = sum([
        urgency_breakdown['error_detection'],
        urgency_breakdown['goal_threatened'],
        urgency_breakdown['agenda_opportunity']
    ])

    mentor_score = sum([
        urgency_breakdown['growth_opportunity'],
        urgency_breakdown['strategic_insight']
    ])

    # å†³ç­–é€»è¾‘
    if critic_score >= 1.5:
        # ä¸¥é‡é”™è¯¯ â†’ Critic Modeä¸»å¯¼
        if mentor_score >= 0.6:
            # ä½†ä¹Ÿæœ‰æ•™å­¦æœºä¼š â†’ Hybridæ¨¡å¼
            return {
                'mode': 'hybrid',
                'primary_pattern': determine_critic_pattern(critic_score),
                'mentorship_layer': 'teaching'
            }
        else:
            # çº¯æ‰¹åˆ¤æ¨¡å¼
            return {
                'mode': 'critic',
                'primary_pattern': determine_critic_pattern(critic_score),
                'mentorship_layer': None
            }

    elif mentor_score >= 1.2:
        # é«˜æˆé•¿æœºä¼šæˆ–æˆ˜ç•¥æ´å¯Ÿ â†’ Mentor Modeä¸»å¯¼
        mentorship_type = 'strategic' if urgency_breakdown['strategic_insight'] > 0.7 else 'teaching'

        return {
            'mode': 'mentor',
            'primary_pattern': determine_mentor_pattern(mentor_score),
            'mentorship_layer': mentorship_type
        }

    else:
        # å¹³è¡¡æ¨¡å¼æˆ–æ²‰é»˜
        total = critic_score + mentor_score

        if total >= 0.85:
            return {
                'mode': 'hybrid',
                'primary_pattern': 'B',
                'mentorship_layer': 'teaching'
            }
        elif total >= 0.35:
            return {
                'mode': 'mentor',
                'primary_pattern': 'M-C',
                'mentorship_layer': 'teaching'
            }
        else:
            return {
                'mode': 'balanced',
                'primary_pattern': 'D',
                'mentorship_layer': None
            }


def determine_critic_pattern(critic_score):
    """
    Criticæ¨¡å¼çš„Patternåˆ†ç±» (V1.2.1)

    Pattern A: urgency >= 0.85
    Pattern B: 0.60 <= urgency < 0.85
    Pattern C: 0.35 <= urgency < 0.60
    Pattern D: urgency < 0.35
    """
    if critic_score >= 0.85:
        return 'A'
    elif critic_score >= 0.60:
        return 'B'
    elif critic_score >= 0.35:
        return 'C'
    else:
        return 'D'


def determine_mentor_pattern(mentor_score):
    """
    Mentoræ¨¡å¼çš„Patternåˆ†ç±» (V2.0æ–°å¢)

    Pattern M-A: ç³»ç»ŸåŒ–æ•™å­¦ (mentor_score >= 1.2)
    Pattern M-B: æŒ‡å¯¼æ€§å»ºè®® (0.8 <= mentor_score < 1.2)
    Pattern M-C: å¯å‘æ€§æç¤º (0.5 <= mentor_score < 0.8)
    Pattern M-D: è§‚å¯Ÿç­‰å¾… (mentor_score < 0.5)
    """
    if mentor_score >= 1.2:
        return 'M-A'
    elif mentor_score >= 0.8:
        return 'M-B'
    elif mentor_score >= 0.5:
        return 'M-C'
    else:
        return 'M-D'
```

### å“åº”æ¨¡å¼çŸ©é˜µ

```
                  Critic Score
                Low         High
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        Low â”‚   D           A/B    â”‚
Mentor      â”‚ (è§‚å¯Ÿ)    (çº é”™ä¸ºä¸») â”‚
Score       â”‚                     â”‚
            â”‚   M-C        Hybrid  â”‚
       High â”‚ (å¯å‘)   (çº é”™+æ•™å­¦) â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Hybridæ¨¡å¼è®¾è®¡

### æ— ç¼è¿‡æ¸¡æ¨¡æ¿

Hybridæ¨¡å¼çš„æ ¸å¿ƒæ˜¯"å…ˆçº é”™ï¼Œåæ•™å­¦"çš„æ— ç¼è¿‡æ¸¡ï¼š

```markdown
âš ï¸ **[Critic] é—®é¢˜è¯†åˆ«**
{detected_error}

ğŸ’¡ **[Mentor] æ¦‚å¿µè§£é‡Š**
ä¸ºä»€ä¹ˆè¿™æ ·æ˜¯æœ‰é—®é¢˜çš„ï¼Ÿ
{conceptual_explanation}

âœ… **[Mentor] æ­£ç¡®åšæ³•**
æ¨èçš„æ–¹æ³•/è¡¨è¿°æ˜¯ï¼š
{correct_approach}

ğŸ“š **[Mentor] å»¶ä¼¸å­¦ä¹ **
ç›¸å…³çŸ¥è¯†ç‚¹ï¼š
{learning_resources}
```

### å®é™…ç¤ºä¾‹

```markdown
âš ï¸ **[Critic] æ–¹æ³•é€‰æ‹©ä¸å½“**
æ‚£ç—…ç‡>10%æ—¶ï¼ŒLogistic regressionçš„ORä¼šé«˜ä¼°å…³è”å¼ºåº¦ã€‚

ğŸ’¡ **[Mentor] OR vs RRçš„åŒºåˆ«**
- OR (Odds Ratio): Logistic regressionè¾“å‡ºï¼Œoddsçš„æ¯”å€¼
- RR (Risk Ratio): é£é™©çš„æ¯”å€¼ï¼Œæ›´ç›´è§‚

å½“æ‚£ç—…ç‡>10%æ—¶ï¼ŒORâ‰ RRï¼Œä¸”ORä¼šå¤¸å¤§æ•ˆåº”ã€‚

ä¾‹å¦‚ï¼šçœŸå®RR=1.5ï¼ŒORå¯èƒ½=2.0

âœ… **[Mentor] æ­£ç¡®åšæ³•**
ä½¿ç”¨log-binomial regressionä¼°è®¡RRï¼š

```r
model <- glm(outcome ~ exposure + covariates,
             family = binomial(link = "log"),
             data = data)
```

å¦‚æœlog-binomialä¸æ”¶æ•›ï¼Œå¤‡é€‰æ–¹æ¡ˆï¼š
- Poisson regression with robust SE
- Modified Poisson approach

ğŸ“š **[Mentor] å»¶ä¼¸å­¦ä¹ **
- Zou G. Modified Poisson regression. Am J Epidemiol. 2004
- ç†è§£ä½•æ—¶ORå¯ä»¥è¿‘ä¼¼RRï¼ˆç½•è§ç–¾ç—…å‡è®¾ï¼‰
```

---

## å®Œæ•´å†³ç­–æµç¨‹ç¤ºä¾‹

### åœºæ™¯: ç”¨æˆ·å’¨è¯¢ç ”ç©¶è®¾è®¡

```python
user_message = """
æˆ‘æƒ³ç ”ç©¶ç¤¾äº¤åª’ä½“ä½¿ç”¨æ˜¯å¦å¯¼è‡´é’å°‘å¹´æŠ‘éƒã€‚
è®¡åˆ’æ”¶é›†1000åé’å°‘å¹´çš„é—®å·æ•°æ®ï¼Œæµ‹é‡ç¤¾äº¤åª’ä½“ä½¿ç”¨æ—¶é—´å’ŒæŠ‘éƒç—‡çŠ¶ã€‚
ç”¨logistic regressionåˆ†æã€‚
ä¸çŸ¥é“è¿™ä¸ªè®¾è®¡æ˜¯å¦åˆé€‚ï¼Ÿ
"""

# ========== Step 1: 8å› å­åˆ†æ ==========

factors = {}

# Factor 1: Error Detection
# - "å¯¼è‡´" (causal language) in observational design
# - Missing confounding control discussion
factors['error_detection'] = 0.75

# Factor 2: Goal Threatened
# - goal_causal_language_precision threatened
# - Related topics: ["causal", "å¯¼è‡´"]
factors['goal_threatened'] = 0.90 * 0.8 = 0.72

# Factor 3: Expertise Match
# - Study design, statistical analysis
factors['expertise_match'] = 0.6

# Factor 4: Misrepresented
factors['misrepresented'] = 0.0

# Factor 5: Silence Too Long
factors['silence_too_long'] = 0.0  # First turn

# Factor 6: Agenda Opportunity
# - agenda_causal_inference_rigor
factors['agenda_opportunity'] = 0.90 (importance)

# Factor 7: Growth Opportunity â­
# - User expressed "ä¸çŸ¥é“...æ˜¯å¦åˆé€‚" (uncertainty)
# - Decision point: study_design
# - Learning intent implied
factors['growth_opportunity'] = 0.8 + 0.6 = 1.0 (capped)

# Factor 8: Strategic Insight â­
# - Research planning ("æƒ³ç ”ç©¶...")
# - Could provide idea assessment
factors['strategic_insight'] = 0.8

# ========== Step 2: è®¡ç®—Urgency ==========

urgency_breakdown = {
    'error_detection': 0.75,
    'goal_threatened': 0.72,
    'expertise_match': 0.6,
    'misrepresented': 0.0,
    'silence_too_long': 0.0,
    'agenda_opportunity': 0.90,
    'growth_opportunity': 1.0,
    'strategic_insight': 0.8
}

# Base urgency (6å› å­)
base = 0.75*0.9 + 0.72*0.8 + 0.6*0.6 + 0 + 0 + 0.90*0.75
     = 0.675 + 0.576 + 0.36 + 0 + 0 + 0.675
     = 2.286

# Mentorship urgency (2å› å­)
mentor = 1.0*0.7 + 0.8*0.65
       = 0.7 + 0.52
       = 1.22

# Total
total_urgency = 2.286 + 1.22 = 3.51

# ========== Step 3: æ¨¡å¼é€‰æ‹© ==========

critic_score = 0.75 + 0.72 + 0.90 = 2.37  # High!
mentor_score = 1.0 + 0.8 = 1.8            # High!

# â†’ Hybrid Mode
selected_mode = {
    'mode': 'hybrid',
    'primary_pattern': 'A',  # critic_score >= 1.5
    'mentorship_layer': 'teaching'  # mentor_score >= 0.6
}

# ========== Step 4: ç”Ÿæˆå“åº” ==========
```

**å“åº”è¾“å‡º**:

```markdown
âš ï¸ **[Critic] ç ”ç©¶è®¾è®¡é—®é¢˜**

æ‚¨çš„è®¾è®¡æœ‰ä¸¤ä¸ªéœ€è¦è°ƒæ•´çš„åœ°æ–¹ï¼š

1. **å› æœè¯­è¨€é—®é¢˜**: "æ˜¯å¦å¯¼è‡´"æš—ç¤ºå› æœå…³ç³»ï¼Œä½†æ¨ªæ–­é¢é—®å·è°ƒæŸ¥æ— æ³•æ”¯æŒå› æœæ¨æ–­
2. **æ··æ‚æ§åˆ¶**: æœªæåŠå¦‚ä½•æ§åˆ¶æ··æ‚å› ç´ ï¼ˆå¦‚å®¶åº­èƒŒæ™¯ã€å­¦ä¸šå‹åŠ›ç­‰ï¼‰

ğŸ’¡ **[Mentor] ä¸ºä»€ä¹ˆè¿™æ ·æœ‰é—®é¢˜ï¼Ÿ**

**å› æœæ¨æ–­çš„ä¸‰ä¸ªåŸºæœ¬æ¡ä»¶**ï¼š
1. æ—¶åºæ€§ï¼šXåœ¨Yä¹‹å‰å‘ç”Ÿ
2. å…³è”æ€§ï¼šXå’ŒYæœ‰ç»Ÿè®¡å…³è”
3. æ’é™¤æ··æ‚ï¼šæ²¡æœ‰ZåŒæ—¶å½±å“Xå’ŒY

æ¨ªæ–­é¢è®¾è®¡çš„å±€é™ï¼š
- âŒ æ—¶åºæ€§ï¼šåŒæ—¶æµ‹é‡ï¼Œæ— æ³•ç¡®å®šè°å…ˆè°å
- âœ“ å…³è”æ€§ï¼šå¯ä»¥æ£€éªŒ
- âš ï¸ æ··æ‚ï¼šéš¾ä»¥å®Œå…¨æ§åˆ¶ï¼ˆåªèƒ½è°ƒæ•´æµ‹é‡åˆ°çš„æ··æ‚ï¼‰

âœ… **[Mentor] ä¸‰ç§æ”¹è¿›æ–¹æ¡ˆ**

**æ–¹æ¡ˆ1: ä¿®æ”¹è¯­è¨€ï¼ˆæœ€ç®€å•ï¼‰**
- ä¸è¯´"å¯¼è‡´"ï¼Œæ”¹ä¸º"å…³è”"
- ç›®çš„ï¼šæ¢ç´¢ç¤¾äº¤åª’ä½“ä½¿ç”¨ä¸æŠ‘éƒç—‡çŠ¶çš„å…³è”
- ç»Ÿè®¡ï¼šLogistic regressionå¯ä»¥ç”¨ï¼Œä½†ç»“è®ºè°¨æ…

**æ–¹æ¡ˆ2: å‡çº§ä¸ºçºµå‘è®¾è®¡ï¼ˆæ¨èï¼‰**
- Baselineæµ‹é‡ï¼šç¤¾äº¤åª’ä½“ä½¿ç”¨
- Follow-upï¼ˆ6ä¸ªæœˆåï¼‰ï¼šæŠ‘éƒç—‡çŠ¶
- è¿™æ ·å¯ä»¥å»ºç«‹æ—¶åºæ€§
- ç»Ÿè®¡ï¼šMixed modelsæˆ–GEE

**æ–¹æ¡ˆ3: ä½¿ç”¨å› æœæ¨æ–­æ–¹æ³•**
å³ä½¿æ˜¯æ¨ªæ–­é¢ï¼Œä¹Ÿå¯ä»¥ï¼š
- æ„å»ºDAGï¼ˆæœ‰å‘æ— ç¯å›¾ï¼‰è¯†åˆ«æ··æ‚
- ä½¿ç”¨å€¾å‘æ€§è¯„åˆ†åŒ¹é…ï¼ˆPSMï¼‰
- è¿›è¡Œæ•æ„Ÿæ€§åˆ†æ

ğŸ¯ **[Strategic Advisor] ç ”ç©¶å½±å“åŠ›è¯„ä¼°**

æ‚¨çš„ç ”ç©¶topicå¾ˆrelevantï¼ˆç¤¾äº¤åª’ä½“ä¸å¿ƒç†å¥åº·ï¼‰ã€‚

**åˆ›æ–°æ€§è¯„ä¼°**ï¼š
- æ¦‚å¿µï¼šMediumï¼ˆè¯¥ä¸»é¢˜å·²æœ‰ç ”ç©¶ï¼‰
- æ–¹æ³•ï¼šå¯ä»¥æå‡ï¼ˆç”¨çºµå‘è®¾è®¡æˆ–å› æœæ–¹æ³•ï¼‰
- äººç¾¤ï¼šçœ‹æ‚¨çš„æ ·æœ¬ç‰¹å¾

**å½±å“åŠ›å»ºè®®**ï¼š
- å¦‚æœæ˜¯pilot studyï¼šæ¨ªæ–­é¢å¯ä»¥æ¥å—ï¼Œæ˜ç¡®æ ‡æ³¨exploratory
- å¦‚æœç›®æ ‡æ˜¯top-tier journalï¼šå»ºè®®çºµå‘è®¾è®¡
- Differentiation: å¯ä»¥focus onç‰¹å®šæœºåˆ¶ï¼ˆå¦‚sleep mediationï¼‰

ğŸ“š **[Mentor] å»¶ä¼¸å­¦ä¹ **

æ¨èèµ„æºï¼š
1. å› æœæ¨æ–­å…¥é—¨ï¼šHernÃ¡n & Robins. Causal Inference What If. (å…è´¹åœ¨çº¿)
2. DAGæ„å»ºï¼šDAGitty (http://dagitty.net)
3. ç±»ä¼¼ç ”ç©¶ï¼šTwenge et al. (2018) on social media and wellbeing

æ‚¨æƒ³ä»å“ªä¸ªæ–¹æ¡ˆå…¥æ‰‹ï¼Ÿæˆ‘å¯ä»¥æä¾›æ›´è¯¦ç»†çš„æŒ‡å¯¼ã€‚
```

---

## æ€§èƒ½ç›‘æ§

### V2.0æ–°å¢æŒ‡æ ‡

```yaml
v2_performance_metrics:

  mode_distribution:
    healthy_range:
      critic_only: 0.30-0.40
      mentor_only: 0.20-0.30
      hybrid: 0.25-0.35
      silence: 0.10-0.20

    alerts:
      if_critic_only_gt_50: "å¯èƒ½è¿‡åº¦æ‰¹åˆ¤ï¼Œç¼ºå°‘æŒ‡å¯¼"
      if_mentor_only_gt_40: "å¯èƒ½å¿½è§†é”™è¯¯æ£€æµ‹"

  mentorship_effectiveness:
    - metric: "Growth opportunity capture rate"
      target: ">85%"
      description: "æ£€æµ‹åˆ°çš„å­¦ä¹ æœºä¼šä¸­æä¾›æŒ‡å¯¼çš„æ¯”ä¾‹"

    - metric: "Strategic insight adoption"
      target: ">70%"
      description: "æˆ˜ç•¥å»ºè®®è¢«ç”¨æˆ·é‡‡çº³çš„æ¯”ä¾‹"

    - metric: "Hybrid mode smoothness"
      target: ">90%"
      description: "ç”¨æˆ·è®¤ä¸ºCriticâ†’Mentorè¿‡æ¸¡è‡ªç„¶çš„æ¯”ä¾‹"
```

---

## æ€»ç»“

V2.0å†³ç­–é€»è¾‘çš„æ ¸å¿ƒæ”¹è¿›ï¼š

1. âœ… **8å› å­ç³»ç»Ÿ**: æ–°å¢growth_opportunityå’Œstrategic_insight
2. âœ… **åŒæ¨¡å¼æ”¯æŒ**: Critic + Mentor + Hybrid
3. âœ… **æ™ºèƒ½åˆ‡æ¢**: åŸºäºurgency breakdownè‡ªåŠ¨é€‰æ‹©æ¨¡å¼
4. âœ… **æ— ç¼è¿‡æ¸¡**: Hybridæ¨¡å¼çš„"çº é”™â†’æ•™å­¦"æµç¨‹
5. âœ… **å¯è§‚æµ‹æ€§**: å®Œæ•´çš„æ€§èƒ½ç›‘æ§æŒ‡æ ‡

**å‘åå…¼å®¹**: V1.2.1çš„æ‰€æœ‰åŠŸèƒ½å®Œå…¨ä¿ç•™ï¼ŒV2.0æ˜¯çº¯å¢é‡å‡çº§ã€‚

---

**Version**: 2.1.0
**Last Updated**: 2025-11-16
**Status**: Production Ready (V2.1 with Memory System)
**Dependencies**:
- decision_logic_guide.md (V1.2.1)
- beliefs.yaml (V2.0)
- writing_guidance.yaml (V2.0)
- strategic_thinking.yaml (V2.0)
- mentorship_goals.yaml (V2.0)
- memory_system.yaml (V2.1) â­ NEW
- memory_operations_guide.md (V2.1) â­ NEW

---

# V2.1 Extension: Hooks Lifecycle Integration

**æ–°å¢æ—¥æœŸ**: 2025-11-16
**æ ¸å¿ƒåŠŸèƒ½**: Pre/Post Hooksè‡ªåŠ¨åŒ–ã€å†…å­˜ç³»ç»Ÿé›†æˆã€æŒç»­å­¦ä¹ 

**çµæ„Ÿæ¥æº**: Claude-Flow v2.7.0çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†æœºåˆ¶

---

## æ¦‚è¿°ï¼šä»V2.0åˆ°V2.1

```
V2.0 (æ— çŠ¶æ€å†³ç­–)                V2.1 (æœ‰è®°å¿†å†³ç­–)
    â†“                                   â†“
ç”¨æˆ·æ¶ˆæ¯ â†’ å†³ç­– â†’ å“åº”          Pre â†’ å†³ç­– â†’ Post
                                 â†“            â†“
                             ä¸Šä¸‹æ–‡å¢å¼º      å­¦ä¹ æå–
                             â†“            â†“
                         å†…å­˜ç³»ç»Ÿ â†â†’ æŒä¹…åŒ–å­˜å‚¨
```

### V2.1æ ¸å¿ƒæ”¹è¿›

1. **Pre-Guidance Phase**: å“åº”å‰è‡ªåŠ¨åŠ è½½ç›¸å…³å†å²
2. **Post-Guidance Phase**: å“åº”åè‡ªåŠ¨å­¦ä¹ å’Œæ›´æ–°
3. **Memory Integration**: è·¨ä¼šè¯å­¦ä¹ å’Œä¸ªæ€§åŒ–
4. **Quality Assurance**: è‡ªåŠ¨è´¨é‡æ£€æŸ¥

---

## Pre-Guidance Phase

### ç›®çš„

åœ¨ç”Ÿæˆå“åº”**ä¹‹å‰**ï¼Œè‡ªåŠ¨ä»å†…å­˜ç³»ç»ŸåŠ è½½ç›¸å…³ä¸Šä¸‹æ–‡ï¼Œä½¿å†³ç­–æ›´åŠ informedã€‚

### å®Œæ•´æµç¨‹

```python
def pre_guidance_phase(user_message, user_id, session_id):
    """
    Pre-Guidanceé˜¶æ®µï¼šä¸Šä¸‹æ–‡å¢å¼º
    åœ¨calculate_urgency_v2ä¹‹å‰è°ƒç”¨

    å‚è€ƒ: memory_operations_guide.md::pre_guidance_context_enrichment
    """

    enriched_context = {}

    # Step 1: Load User Profile
    # ä»SQLite user_profilesè¡¨åŠ è½½èƒ½åŠ›ç”»åƒ
    enriched_context['user_profile'] = load_user_profile(user_id)

    # å…³é”®å­—æ®µ:
    # - overall_level: novice/intermediate/advanced
    # - skill_study_design, skill_statistics, skill_writing, skill_critical_appraisal
    # - current_learning_focus
    # - preferred_mode, response_depth_preference

    # Step 2: Retrieve Recent Interactions (æœ€è¿‘5æ¬¡å¯¹è¯)
    # ç”¨äºç†è§£å¯¹è¯ä¸Šä¸‹æ–‡å’Œè¿ç»­æ€§
    enriched_context['recent_history'] = query_sql("""
        SELECT user_message, guidance_response, mode_used, timestamp
        FROM user_interactions
        WHERE user_id = ?
        ORDER BY timestamp DESC
        LIMIT 5
    """, [user_id])

    # Step 3: Check Recurring Errors
    # æ£€æµ‹è¿‡å»30å¤©å†…é‡å¤å‡ºç°çš„é”™è¯¯æ¨¡å¼
    enriched_context['recurring_errors'] = detect_recurring_errors(user_id, lookback_days=30)

    # recurring_errorsæ ¼å¼:
    # [
    #   {
    #     'error_type': 'multiple_comparison_no_correction',
    #     'occurrence_count': 3,
    #     'last_occurrence': '2025-11-10',
    #     'recommended_strategies': [...]  # ä»guidance_casesæ£€ç´¢çš„æœ€ä½³çº æ­£ç­–ç•¥
    #   }
    # ]

    # Step 4: Semantic Search for Similar Success Cases
    # ä»ChromaDB guidance_cases collectionæ£€ç´¢ç›¸ä¼¼çš„æˆåŠŸæŒ‡å¯¼æ¡ˆä¾‹
    try:
        enriched_context['similar_success_cases'] = chromadb_semantic_search(
            collection="guidance_cases",
            query=user_message,
            filters={
                "user_level": enriched_context['user_profile'].overall_level,
                "effectiveness_score": {"$gte": 0.8}
            },
            top_k=3
        )
    except ChromaDBException:
        # Fallback to SQLite keyword matching
        enriched_context['similar_success_cases'] = sqlite_keyword_search(
            user_message,
            user_level=enriched_context['user_profile'].overall_level
        )

    # similar_success_casesæ ¼å¼:
    # [
    #   {
    #     'problem_type': 'study_design_selection',
    #     'user_message': '...',
    #     'guidance_template': '...',
    #     'effectiveness_score': 0.92,
    #     'similarity_score': 0.87
    #   }
    # ]

    # Step 5: Identify Current Learning Focus
    # ä»skill_progressè¡¨è¯†åˆ«ç”¨æˆ·å½“å‰å­¦ä¹ é‡ç‚¹
    enriched_context['current_focus'] = query_sql("""
        SELECT skill_domain, skill_name, current_level
        FROM skill_progress
        WHERE user_id = ?
        ORDER BY advancement_date DESC
        LIMIT 1
    """, [user_id])

    # Step 6: Estimate Task Complexity (ä¸ºPhase 3å‡†å¤‡)
    enriched_context['estimated_complexity'] = estimate_task_complexity(
        user_message=user_message,
        user_profile=enriched_context['user_profile']
    )

    log(f"[Pre-Guidance] Context enriched for session {session_id}")
    log(f"  â€¢ User level: {enriched_context['user_profile'].overall_level}")
    log(f"  â€¢ Recurring errors: {len(enriched_context['recurring_errors'])}")
    log(f"  â€¢ Similar cases found: {len(enriched_context['similar_success_cases'])}")

    return enriched_context
```

### é›†æˆåˆ°V2.0å†³ç­–æµç¨‹

```python
# ä¿®æ”¹åçš„calculate_urgency_v2å‡½æ•°

def calculate_urgency_v2_enhanced(user_message, user_id, session_id):
    """
    V2.1å¢å¼ºç‰ˆurgencyè®¡ç®—
    é›†æˆPre-Guidanceä¸Šä¸‹æ–‡å¢å¼º
    """

    # ğŸ†• V2.1: Pre-Guidance Phase
    enriched_context = pre_guidance_phase(user_message, user_id, session_id)

    # V2.0: 8-factoræ£€æµ‹ (ç°åœ¨å¯ä»¥ä½¿ç”¨enriched_context)
    factors = {
        'error_detection': detect_error(user_message),

        'goal_threatened': check_goal_threat(user_message),

        'expertise_match': calculate_expertise_match(user_message),

        'misrepresented': detect_misrepresentation(user_message),

        'silence_too_long': calculate_silence_duration(enriched_context['recent_history']),  # ğŸ†• ä½¿ç”¨å†å²

        'agenda_opportunity': detect_agenda_opportunity(user_message),

        # ğŸ†• V2.1: ä½¿ç”¨enriched_contextå¢å¼ºæ£€æµ‹
        'growth_opportunity': detect_growth_opportunity_enhanced(
            user_message,
            user_profile=enriched_context['user_profile'],
            recurring_errors=enriched_context['recurring_errors'],
            current_focus=enriched_context['current_focus']
        ),

        'strategic_insight': detect_strategic_insight_enhanced(
            user_message,
            user_profile=enriched_context['user_profile'],
            recent_history=enriched_context['recent_history']
        )
    }

    # ğŸ†• V2.1: æ ¹æ®é‡å¤é”™è¯¯åŠ¨æ€è°ƒæ•´æƒé‡
    weights = get_decision_weights()

    if enriched_context['recurring_errors']:
        # é‡å¤é”™è¯¯æ£€æµ‹åˆ°ï¼Œå¼ºåŒ–error_detectionå’Œgrowth_opportunity
        weights['error_detection'] *= 1.2
        weights['growth_opportunity'] *= 1.3
        log("[Weight Boost] Recurring errors detected, boosting correction weights")

    # ğŸ†• V2.1: æ ¹æ®ç”¨æˆ·å†å²é€‰æ‹©æ¨¡å¼
    recommended_mode = select_mode_from_context(enriched_context)
    adjusted_weights = apply_mode_adjustments(weights, recommended_mode)

    # è®¡ç®—urgency
    urgency = calculate_weighted_sum(factors, adjusted_weights)

    return {
        'urgency': urgency,
        'factors': factors,
        'mode': recommended_mode,
        'enriched_context': enriched_context  # ğŸ†• ä¼ é€’ç»™response generation
    }
```

### å¢å¼ºçš„Factoræ£€æµ‹

```python
def detect_growth_opportunity_enhanced(user_message, user_profile, recurring_errors, current_focus):
    """
    V2.1å¢å¼ºç‰ˆgrowth_opportunityæ£€æµ‹
    åˆ©ç”¨å†…å­˜ç³»ç»Ÿçš„ä¸Šä¸‹æ–‡
    """

    score = 0.0
    opportunities = []

    # åŸæœ‰çš„V2.0æ£€æµ‹é€»è¾‘ (uncertainty, decision_points, learning_intent)
    base_score, base_opps = detect_growth_opportunity(user_message, user_profile, None)
    score += base_score
    opportunities.extend(base_opps)

    # ğŸ†• V2.1: åŸºäºrecurring_errorsçš„æ£€æµ‹
    if recurring_errors:
        for error in recurring_errors:
            # æ£€æŸ¥å½“å‰æ¶ˆæ¯æ˜¯å¦ä¸é‡å¤é”™è¯¯ç›¸å…³
            if is_message_related_to_error(user_message, error['error_type']):
                score += 0.95  # æœ€é«˜ä¼˜å…ˆçº§
                opportunities.append(f"recurring_error_{error['error_type']}_count_{error['occurrence_count']}")

                log(f"[Growth Opp] Recurring error detected: {error['error_type']} ({error['occurrence_count']} times)")
                break

    # ğŸ†• V2.1: åŸºäºcurrent_focusçš„æ£€æµ‹
    if current_focus:
        focus_domain = current_focus['skill_domain']
        # å¦‚æœç”¨æˆ·æ¶ˆæ¯æ¶‰åŠå½“å‰å­¦ä¹ é‡ç‚¹ï¼Œè§†ä¸ºæˆé•¿æœºä¼š
        if is_message_related_to_domain(user_message, focus_domain):
            score += 0.7
            opportunities.append(f"aligned_with_current_focus_{focus_domain}")

            log(f"[Growth Opp] Message aligned with current focus: {focus_domain}")

    return min(score, 1.0), opportunities
```

---

## Post-Guidance Phase

### ç›®çš„

åœ¨ç”Ÿæˆå“åº”**ä¹‹å**ï¼Œè‡ªåŠ¨è¯„ä¼°è´¨é‡ã€æå–å­¦ä¹ ç‚¹ã€æ›´æ–°å†…å­˜ç³»ç»Ÿã€‚

### å®Œæ•´æµç¨‹

```python
def post_guidance_phase(user_message, guidance_response, decision_result, user_id, session_id):
    """
    Post-Guidanceé˜¶æ®µï¼šå­¦ä¹ æå–å’Œå†…å­˜æ›´æ–°
    åœ¨è¿”å›å“åº”ç»™ç”¨æˆ·ä¹‹åè°ƒç”¨ï¼ˆå¼‚æ­¥ï¼‰

    å‚è€ƒ: memory_operations_guide.md::post_guidance_learning_extraction
    """

    learning_results = {}

    # Step 1: Quality Self-Check
    quality_score = evaluate_guidance_quality(
        guidance_response=guidance_response,
        decision_result=decision_result,
        enriched_context=decision_result['enriched_context']
    )

    learning_results['quality_score'] = quality_score

    if quality_score < 0.6:
        log(f"âš ï¸ [Quality] Low quality guidance detected (score={quality_score:.2f})")
        # æ ‡è®°ä¸ºéœ€è¦æ”¹è¿›ï¼Œæœªæ¥å¯è§¦å‘äººå·¥å®¡æ ¸

    log(f"âœ“ [Quality] Self-check completed: {quality_score:.2f}")

    # Step 2: Extract Learning Insights
    insights = extract_learning_insights(
        user_message=user_message,
        guidance_response=guidance_response,
        enriched_context=decision_result['enriched_context']
    )

    learning_results['insights'] = insights

    # insightsæ ¼å¼:
    # {
    #   'problem_type': 'study_design_selection',
    #   'skill_demonstrated': ['understanding_RCT', 'identify_confounders'],
    #   'skill_advancement': True,
    #   'new_level': 0.6,
    #   'advancement_evidence': 'ç”¨æˆ·æ­£ç¡®è¯†åˆ«äº†æ··æ‚å› ç´ ',
    #   'user_confusion_points': ['unclear about propensity score']
    # }

    # Step 3: Update Skill Progress (if advancement detected)
    if insights['skill_advancement']:
        update_skill_progress(
            user_id=user_id,
            skill_domain=insights['skill_domain'],
            new_level=insights['new_level'],
            evidence=insights['advancement_evidence']
        )

        log(f"ğŸ“ [Skill Up] {insights['skill_domain']} â†’ {insights['new_level']}")

    # Step 4: Update User Profile Statistics
    update_user_profile_stats(
        user_id=user_id,
        total_interactions=1,  # increment
        errors_detected=len(decision_result['factors']['error_detection']),
        guidance_provided=1
    )

    # Step 5: Store Interaction to Memory
    interaction_record = {
        "session_id": session_id,
        "user_id": user_id,
        "user_message": user_message,
        "guidance_response": guidance_response,
        "mode_used": decision_result['mode'],
        "complexity_score": decision_result['enriched_context']['estimated_complexity'],
        "quality_score": quality_score,
        "timestamp": now()
    }

    # å­˜å…¥SQLite user_interactionsè¡¨
    insert_into_table("user_interactions", interaction_record)

    # ğŸ†• V2.1: å­˜å…¥ChromaDB (ä»…å­˜å‚¨ä¸­é«˜è´¨é‡äº¤äº’)
    if quality_score >= 0.7:
        add_to_chromadb_async(
            collection="user_interactions",
            document=user_message + "\n" + guidance_response,
            metadata=interaction_record
        )

    log(f"âœ“ [Storage] Interaction stored (quality={quality_score:.2f})")

    # Step 6: Store as Guidance Case (ä»…é«˜è´¨é‡æ¡ˆä¾‹)
    if quality_score >= 0.85:
        guidance_case = {
            "case_id": generate_case_id(),
            "problem_type": insights['problem_type'],
            "user_level": decision_result['enriched_context']['user_profile'].overall_level,
            "guidance_strategy": decision_result['mode'],
            "effectiveness_score": quality_score,
            "user_message": user_message,
            "guidance_template": extract_template(guidance_response),
            "tags": extract_tags(insights)
        }

        add_to_chromadb(
            collection="guidance_cases",
            document=guidance_response,
            metadata=guidance_case
        )

        log(f"âœ¨ [Best Practice] Stored as high-quality guidance case (score={quality_score:.2f})")

    # Step 7: Pattern Learning (ä¸ºV2.5 Neural Learningå‡†å¤‡)
    # è®°å½• (é—®é¢˜ç±»å‹, ç­–ç•¥, æ•ˆæœ) ä¸‰å…ƒç»„
    store_pattern_triple(
        problem_type=insights['problem_type'],
        strategy=decision_result['mode'],
        effectiveness=quality_score
    )

    learning_results['stored'] = True

    log(f"âœ“ [Post-Guidance] Learning extraction completed for session {session_id}")

    return learning_results
```

---

## Quality Self-Check

### è‡ªåŠ¨è´¨é‡è¯„ä¼°æ ‡å‡†

```python
def evaluate_guidance_quality(guidance_response, decision_result, enriched_context):
    """
    è‡ªåŠ¨è¯„ä¼°ç”Ÿæˆçš„guidanceè´¨é‡
    åŸºäºå¤šç»´åº¦æ£€æŸ¥

    å‚è€ƒ: CLAUDE_FLOW_INSIGHTS.md::Phase 2::quality_check
    """

    score = 1.0  # åˆå§‹æ»¡åˆ†
    issues = []

    # Check 1: æ˜¯å¦å¼•ç”¨äº†å…·ä½“æ ‡å‡†/æ–‡çŒ®? (æƒé‡: 0.15)
    has_references = check_for_references(guidance_response)
    # æ£€æµ‹å…³é”®è¯: "CONSORT", "STROBE", "et al.", "2023", "ç ”ç©¶æ˜¾ç¤º"
    if not has_references:
        score -= 0.15
        issues.append("missing_references")

    # Check 2: æ˜¯å¦æä¾›äº†å¯æ“ä½œå»ºè®®? (æƒé‡: 0.20)
    has_actionable = check_for_actionable_advice(guidance_response)
    # æ£€æµ‹: "å»ºè®®", "å¯ä»¥", "åº”è¯¥", "æ­¥éª¤", "æ–¹æ³•"
    if not has_actionable:
        score -= 0.20
        issues.append("missing_actionable_advice")

    # Check 3: æ˜¯å¦åŒ¹é…ç”¨æˆ·èƒ½åŠ›æ°´å¹³? (æƒé‡: 0.15)
    user_level = enriched_context['user_profile'].overall_level
    complexity_match = check_complexity_match(guidance_response, user_level)

    # novice: é¿å…è¿‡åº¦æŠ€æœ¯æœ¯è¯­
    # intermediate: å¹³è¡¡è§£é‡Šä¸ä¸“ä¸šæ€§
    # advanced: å¯ä»¥ä½¿ç”¨é«˜çº§æ¦‚å¿µ

    if not complexity_match:
        score -= 0.15
        issues.append("complexity_mismatch")

    # Check 4: æ˜¯å¦å›ç­”äº†ç”¨æˆ·çš„å®é™…é—®é¢˜? (æƒé‡: 0.20)
    relevance_score = calculate_semantic_relevance(
        user_message=decision_result['user_message'],
        guidance_response=guidance_response
    )

    if relevance_score < 0.7:
        score -= 0.20
        issues.append("low_relevance")

    # Check 5: è¯­è¨€æ˜¯å¦professionalä¸”constructive? (æƒé‡: 0.10)
    tone_analysis = analyze_tone(guidance_response)
    # æ£€æµ‹: æ˜¯å¦è¿‡äºä¸¥å‰ã€æ˜¯å¦æœ‰å»ºè®¾æ€§ã€æ˜¯å¦æœ‰é¼“åŠ±

    if tone_analysis != "professional_constructive":
        score -= 0.10
        issues.append(f"tone_issue_{tone_analysis}")

    # Check 6: æ˜¯å¦åˆ©ç”¨äº†similar_success_cases? (æƒé‡: 0.10)
    # ğŸ†• V2.1: æ£€æŸ¥æ˜¯å¦æœ‰æ•ˆåˆ©ç”¨äº†æ£€ç´¢åˆ°çš„æˆåŠŸæ¡ˆä¾‹
    similar_cases = enriched_context.get('similar_success_cases', [])
    if similar_cases and not check_case_utilization(guidance_response, similar_cases):
        score -= 0.10
        issues.append("underutilized_similar_cases")

    # Check 7: æ˜¯å¦é’ˆå¯¹recurring_errorsæä¾›æ·±åº¦æŒ‡å¯¼? (æƒé‡: 0.10)
    # ğŸ†• V2.1: å¦‚æœæ£€æµ‹åˆ°é‡å¤é”™è¯¯ï¼Œå¿…é¡»æä¾›æ·±åº¦æ•™å­¦
    recurring_errors = enriched_context.get('recurring_errors', [])
    if recurring_errors and decision_result['factors']['growth_opportunity'] > 0.9:
        # åº”è¯¥åŒ…å«: æ¦‚å¿µæ¡†æ¶ã€å¤šä¸ªä¾‹å­ã€ç»ƒä¹ é¢˜
        has_deep_teaching = check_deep_teaching_components(guidance_response)
        if not has_deep_teaching:
            score -= 0.10
            issues.append("shallow_teaching_for_recurring_error")

    final_score = max(score, 0.0)

    if issues:
        log(f"[Quality] Issues detected: {', '.join(issues)}")

    return final_score
```

### Quality Checkè¾…åŠ©å‡½æ•°

```python
def check_for_references(text):
    """æ£€æŸ¥æ˜¯å¦åŒ…å«æ–‡çŒ®å¼•ç”¨æˆ–æ ‡å‡†"""
    reference_patterns = [
        r'\b(CONSORT|STROBE|PRISMA|TRIPOD|STARD)\b',  # æŠ¥å‘Šè§„èŒƒ
        r'\bet al\.',  # æ–‡çŒ®å¼•ç”¨
        r'\b(19|20)\d{2}\b',  # å¹´ä»½
        r'ç ”ç©¶(æ˜¾ç¤º|è¡¨æ˜|å‘ç°)',  # ä¸­æ–‡ç ”ç©¶å¼•ç”¨
        r'(Journal|Lancet|NEJM|BMJ)',  # æœŸåˆŠå
    ]

    return any(re.search(pattern, text, re.IGNORECASE) for pattern in reference_patterns)


def check_for_actionable_advice(text):
    """æ£€æŸ¥æ˜¯å¦åŒ…å«å¯æ“ä½œçš„å»ºè®®"""
    actionable_keywords = [
        'å»ºè®®', 'å¯ä»¥', 'åº”è¯¥', 'æ­¥éª¤', 'æ–¹æ³•', 'é¦–å…ˆ', 'å…¶æ¬¡',
        'recommend', 'suggest', 'should', 'can', 'step', 'method'
    ]

    return sum(1 for kw in actionable_keywords if kw in text.lower()) >= 2


def check_complexity_match(text, user_level):
    """æ£€æŸ¥å†…å®¹å¤æ‚åº¦æ˜¯å¦åŒ¹é…ç”¨æˆ·æ°´å¹³"""
    # ç®€åŒ–ç‰ˆå®ç°ï¼šç»Ÿè®¡æŠ€æœ¯æœ¯è¯­å¯†åº¦

    advanced_terms = [
        'propensity score', 'instrumental variable', 'causal diagram',
        'marginal structural model', 'g-computation', 'å€¾å‘æ€§è¯„åˆ†', 'å·¥å…·å˜é‡'
    ]

    intermediate_terms = [
        'confounding', 'selection bias', 'regression', 'validation',
        'æ··æ‚', 'åå€š', 'å›å½’', 'éªŒè¯'
    ]

    advanced_count = sum(1 for term in advanced_terms if term in text.lower())
    intermediate_count = sum(1 for term in intermediate_terms if term in text.lower())

    if user_level == 'novice':
        # æ–°æ‰‹ï¼šé«˜çº§æœ¯è¯­åº”å°‘äº2ä¸ª
        return advanced_count < 2

    elif user_level == 'intermediate':
        # ä¸­çº§ï¼šå…è®¸ä¸€äº›é«˜çº§æœ¯è¯­ï¼Œä½†ä¸èƒ½è¿‡å¤š
        return advanced_count < 5

    else:  # advanced
        # é«˜çº§ï¼šå¯ä»¥è‡ªç”±ä½¿ç”¨ä¸“ä¸šæœ¯è¯­
        return True


def calculate_semantic_relevance(user_message, guidance_response):
    """
    è®¡ç®—å“åº”ä¸ç”¨æˆ·é—®é¢˜çš„è¯­ä¹‰ç›¸å…³æ€§
    ç®€åŒ–ç‰ˆï¼šå…³é”®è¯é‡å åº¦
    """
    user_keywords = extract_keywords(user_message)
    response_keywords = extract_keywords(guidance_response)

    overlap = set(user_keywords) & set(response_keywords)
    relevance = len(overlap) / max(len(user_keywords), 1)

    return relevance
```

---

## å®Œæ•´V2.1å†³ç­–æµç¨‹ç¤ºä¾‹

### ç«¯åˆ°ç«¯æµç¨‹

```python
def handle_user_message_v2_1(user_message, user_id, session_id):
    """
    ACS-Mentor V2.1å®Œæ•´å·¥ä½œæµ
    Pre â†’ Decision â†’ Generation â†’ Post
    """

    log(f"\n{'='*60}")
    log(f"Session {session_id} - Processing user message")
    log(f"{'='*60}\n")

    # ========== Phase 1: Pre-Guidance ==========
    log("[Phase 1] Pre-Guidance: Context enrichment...")

    enriched_context = pre_guidance_phase(
        user_message=user_message,
        user_id=user_id,
        session_id=session_id
    )

    log(f"âœ“ Context loaded:")
    log(f"  â€¢ User level: {enriched_context['user_profile'].overall_level}")
    log(f"  â€¢ Recent interactions: {len(enriched_context['recent_history'])}")
    log(f"  â€¢ Recurring errors: {len(enriched_context['recurring_errors'])}")
    log(f"  â€¢ Similar success cases: {len(enriched_context['similar_success_cases'])}")

    # ========== Phase 2: Decision & Urgency Calculation ==========
    log("\n[Phase 2] Decision: Calculating urgency and selecting mode...")

    decision_result = calculate_urgency_v2_enhanced(
        user_message=user_message,
        user_id=user_id,
        session_id=session_id
    )

    log(f"âœ“ Decision made:")
    log(f"  â€¢ Urgency: {decision_result['urgency']:.2f}")
    log(f"  â€¢ Mode: {decision_result['mode']}")
    log(f"  â€¢ Top factors:")
    sorted_factors = sorted(
        decision_result['factors'].items(),
        key=lambda x: x[1] if isinstance(x[1], (int, float)) else 0,
        reverse=True
    )
    for factor, score in sorted_factors[:3]:
        if isinstance(score, (int, float)) and score > 0:
            log(f"    - {factor}: {score:.2f}")

    # ========== Phase 3: Response Generation ==========
    log("\n[Phase 3] Generation: Creating guidance response...")

    # ä½¿ç”¨enriched_contextä¸­çš„similar_success_casesä½œä¸ºæ¨¡æ¿
    guidance_response = generate_guidance_response(
        user_message=user_message,
        decision_result=decision_result,
        template_cases=enriched_context['similar_success_cases']
    )

    log(f"âœ“ Response generated (length: {len(guidance_response)} chars)")

    # ========== Phase 4: Post-Guidance ==========
    log("\n[Phase 4] Post-Guidance: Learning extraction and memory update...")

    learning_results = post_guidance_phase(
        user_message=user_message,
        guidance_response=guidance_response,
        decision_result=decision_result,
        user_id=user_id,
        session_id=session_id
    )

    log(f"âœ“ Learning extracted:")
    log(f"  â€¢ Quality score: {learning_results['quality_score']:.2f}")
    log(f"  â€¢ Skill advancement: {learning_results['insights'].get('skill_advancement', False)}")
    log(f"  â€¢ Stored to memory: {learning_results['stored']}")

    # ========== Phase 5: Skill Advancement Check ==========
    if learning_results['insights'].get('skill_advancement'):
        skill_domain = learning_results['insights']['skill_domain']
        new_level = learning_results['insights']['new_level']

        # åœ¨å“åº”ä¸­æ·»åŠ ç¥è´ºä¿¡æ¯
        celebration_message = f"\n\nğŸ‰ **æ­å–œï¼æ‚¨åœ¨ã€Œ{skill_domain}ã€æ–¹é¢å·²æ™‹çº§åˆ° {new_level} æ°´å¹³ï¼**"
        guidance_response += celebration_message

        log(f"ğŸ“ Skill advancement celebrated: {skill_domain} â†’ {new_level}")

    log(f"\n{'='*60}")
    log(f"Session {session_id} completed")
    log(f"{'='*60}\n")

    return guidance_response
```

### è¿è¡Œç¤ºä¾‹

```
ç”¨æˆ·æ¶ˆæ¯: "æˆ‘åœ¨åšä¸€ä¸ªRCTç ”ç©¶ï¼Œä½†æ˜¯æœ‰20%çš„missing dataï¼Œæ€ä¹ˆå¤„ç†ï¼Ÿ"

============================================================
Session abc123 - Processing user message
============================================================

[Phase 1] Pre-Guidance: Context enrichment...
âœ“ Context loaded:
  â€¢ User level: intermediate
  â€¢ Recent interactions: 3
  â€¢ Recurring errors: 1 (missing_data_deletion_only)
  â€¢ Similar success cases: 2

[Phase 2] Decision: Calculating urgency and selecting mode...
[Weight Boost] Recurring errors detected, boosting correction weights
âœ“ Decision made:
  â€¢ Urgency: 1.45
  â€¢ Mode: hybrid
  â€¢ Top factors:
    - growth_opportunity: 0.95 (recurring_error_missing_data_deletion_only_count_2)
    - expertise_match: 0.80
    - error_detection: 0.60

[Phase 3] Generation: Creating guidance response...
âœ“ Response generated (length: 1850 chars)

[Phase 4] Post-Guidance: Learning extraction and memory update...
[Quality] Self-check completed: 0.88
âœ“ Learning extracted:
  â€¢ Quality score: 0.88
  â€¢ Skill advancement: True
  â€¢ Stored to memory: True
âœ¨ [Best Practice] Stored as high-quality guidance case (score=0.88)
ğŸ“ Skill advancement celebrated: statistical_methods â†’ 0.7

============================================================
Session abc123 completed
============================================================

è¿”å›å“åº”:
```

ğŸ” **[Critic] é”™è¯¯æ£€æµ‹**

æ£€æµ‹åˆ°ç¼ºå¤±æ•°æ®å¤„ç†ä¸å½“ã€‚20%çš„missing dataæ˜¯ä¸èƒ½å¿½ç•¥çš„ï¼ˆé€šå¸¸>5%å°±éœ€è¦ç‰¹åˆ«å¤„ç†ï¼‰ã€‚

âš ï¸ **é‡è¦**: è¿™æ˜¯æ‚¨ç¬¬2æ¬¡åœ¨missing dataå¤„ç†ä¸Šé‡åˆ°é—®é¢˜ã€‚è®©æˆ‘æä¾›æ›´æ·±å…¥çš„æŒ‡å¯¼ã€‚

---

ğŸ“ **[Mentor] æ·±åº¦æ¦‚å¿µæ¡†æ¶**

**ç¼ºå¤±æ•°æ®çš„ä¸‰ç§æœºåˆ¶** (Little & Rubin, 2002):

1. **MCAR** (Missing Completely At Random): ç¼ºå¤±ä¸ä»»ä½•å˜é‡æ— å…³
2. **MAR** (Missing At Random): ç¼ºå¤±ä¸è§‚æµ‹åˆ°çš„å˜é‡ç›¸å…³
3. **MNAR** (Missing Not At Random): ç¼ºå¤±ä¸æœªè§‚æµ‹çš„å˜é‡ç›¸å…³

**ä¸ºä»€ä¹ˆé‡è¦?** ä¸åŒæœºåˆ¶éœ€è¦ä¸åŒå¤„ç†æ–¹æ³•ã€‚

---

ğŸ“‹ **[Mentor] å¤„ç†ç­–ç•¥**

**æ–¹æ³•1: Multiple Imputation (æ¨è) âœ…**
- é€‚ç”¨äº: MARå‡è®¾
- RåŒ…: `mice`
- æ­¥éª¤:
  1. ç”Ÿæˆmä¸ªå®Œæ•´æ•°æ®é›†ï¼ˆé€šå¸¸m=5-20ï¼‰
  2. åˆ†åˆ«åˆ†ææ¯ä¸ªæ•°æ®é›†
  3. åˆå¹¶ç»“æœ (Rubin's rules)

**æ–¹æ³•2: Full Information Maximum Likelihood (FIML)**
- é€‚ç”¨äº: MARå‡è®¾ï¼Œçºµå‘æ•°æ®
- RåŒ…: `lavaan`

**æ–¹æ³•3: Sensitivity Analysis**
- å¯¹äºMNARçš„ä¿å®ˆç­–ç•¥
- æµ‹è¯•ä¸åŒå‡è®¾ä¸‹çš„ç»“æœç¨³å¥æ€§

---

**âŒ ä¸æ¨èçš„æ–¹æ³•**:
- Complete case analysis (listwise deletion) - æŸå¤±powerä¸”å¯èƒ½æœ‰å
- Mean imputation - ä½ä¼°æ ‡å‡†è¯¯

---

ğŸ§ª **[Mentor] å®è·µç»ƒä¹ **

å°è¯•å›ç­”:
1. æ‚¨çš„missing dataæ›´å¯èƒ½æ˜¯MARè¿˜æ˜¯MNARï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ
2. å¦‚æœç¼ºå¤±ä¸»è¦å‘ç”Ÿåœ¨treatmentç»„ï¼Œè¿™æš—ç¤ºä»€ä¹ˆæœºåˆ¶ï¼Ÿ

---

ğŸ“š **å»¶ä¼¸é˜…è¯»**:
- Van Buuren (2018). Flexible Imputation of Missing Data (R `mice`åŒ…ä½œè€…)
- æŠ¥å‘Šè§„èŒƒ: CONSORT 2010 Extension for Missing Data

---

ğŸ‰ **æ­å–œï¼æ‚¨åœ¨ã€Œstatistical_methodsã€æ–¹é¢å·²æ™‹çº§åˆ° 0.7 æ°´å¹³ï¼**

```

---

## V2.1æ€§èƒ½ç›‘æ§æ‰©å±•

### æ–°å¢æŒ‡æ ‡

```yaml
v2_1_performance_metrics:

  memory_system_health:
    - metric: "ChromaDB availability"
      target: "> 99%"
      degradation: "Auto-fallback to SQLite"

    - metric: "Retrieval latency (p95)"
      target: "< 100ms"
      alert_threshold: "> 200ms"

    - metric: "Memory growth rate"
      target: "< 10MB/week"
      alert_threshold: "> 50MB/week"

  context_enrichment_effectiveness:
    - metric: "Similar case retrieval success rate"
      target: "> 80%"
      description: "æ£€ç´¢åˆ°è‡³å°‘1ä¸ªç›¸å…³æ¡ˆä¾‹çš„æ¯”ä¾‹"

    - metric: "Recurring error detection rate"
      target: "> 95%"
      description: "æˆåŠŸæ£€æµ‹åˆ°é‡å¤é”™è¯¯çš„æ¯”ä¾‹"

  guidance_quality:
    - metric: "Average quality score"
      target: "> 0.80"
      alert_threshold: "< 0.70"

    - metric: "High-quality case storage rate"
      target: "15-25%"
      description: "quality >= 0.85çš„æ¡ˆä¾‹å æ¯”"

  learning_effectiveness:
    - metric: "Skill advancement rate"
      target: "å¹³å‡æ¯æœˆè‡³å°‘1æ¬¡æ™‹çº§"
      measurement: "ä»skill_progressè¡¨ç»Ÿè®¡"

    - metric: "Recurring error elimination rate"
      target: "> 60%"
      description: "é‡å¤é”™è¯¯è¢«æˆåŠŸçº æ­£ï¼ˆä¸å†å‡ºç°ï¼‰çš„æ¯”ä¾‹"
```

---

## æ€»ç»“ï¼šV2.1æ ¸å¿ƒä»·å€¼

### ä»V2.0åˆ°V2.1çš„è´¨å˜

| ç»´åº¦ | V2.0 | V2.1 |
|------|------|------|
| **çŠ¶æ€** | æ— çŠ¶æ€ | æœ‰è®°å¿† |
| **ä¸ªæ€§åŒ–** | æ—  | æ·±åº¦ä¸ªæ€§åŒ– |
| **å­¦ä¹ ** | é™æ€çŸ¥è¯†åº“ | æŒç»­å­¦ä¹  |
| **è´¨é‡ä¿è¯** | æ—  | è‡ªåŠ¨è´¨æ£€ |
| **é”™è¯¯å¤„ç†** | å•æ¬¡çº æ­£ | è¿½è¸ªé‡å¤é”™è¯¯ |
| **æ¡ˆä¾‹å¤ç”¨** | æ—  | è¯­ä¹‰æœç´¢æˆåŠŸæ¡ˆä¾‹ |
| **æŠ€èƒ½è¿½è¸ª** | æ‰‹åŠ¨ | è‡ªåŠ¨æ™‹çº§æ£€æµ‹ |

### V2.1å…³é”®èƒ½åŠ›

1. âœ… **Pre-Guidance Context Enrichment**: å“åº”å‰è‡ªåŠ¨åŠ è½½6ç±»ä¸Šä¸‹æ–‡
2. âœ… **Post-Guidance Learning Extraction**: å“åº”åè‡ªåŠ¨å­¦ä¹ å’Œæ›´æ–°
3. âœ… **Recurring Error Detection**: è¯†åˆ«ç”¨æˆ·é‡å¤çŠ¯çš„é”™è¯¯ï¼ˆthreshold=2ï¼‰
4. âœ… **Semantic Case Retrieval**: ä»å†å²æˆåŠŸæ¡ˆä¾‹ä¸­æ£€ç´¢æœ€ä½³æ¨¡æ¿
5. âœ… **Automatic Quality Check**: 7ç»´åº¦è´¨é‡è‡ªåŠ¨è¯„ä¼°
6. âœ… **Skill Progression Tracking**: è‡ªåŠ¨æ£€æµ‹å’Œåº†ç¥æŠ€èƒ½æ™‹çº§
7. âœ… **Memory System Integration**: ChromaDB + SQLiteæ··åˆæ¶æ„

### ä¸Claude-Flowå¯¹æ ‡

| åŠŸèƒ½ | Claude-Flow | ACS-Mentor V2.1 |
|------|-------------|-----------------|
| Pre-Task Context | âœ… å¤æ‚åº¦è¯„ä¼°+ä»»åŠ¡åˆ†é… | âœ… 6-stageä¸Šä¸‹æ–‡å¢å¼º |
| Post-Task Learning | âœ… Neural pattern learning | âœ… 7-checkè´¨é‡è¯„ä¼°+å­¦ä¹ æå– |
| Memory System | âœ… AgentDB+ReasoningBank | âœ… ChromaDB+SQLite |
| Auto-degradation | âœ… Hybrid fallback | âœ… ChromaDBâ†’SQLiteâ†’Stateless |

**V2.1è¾¾æˆClaude-Flowçš„æ ¸å¿ƒå·¥ç¨‹ä»·å€¼**: æœ‰è®°å¿†ã€å¯å­¦ä¹ ã€æŒç»­ä¼˜åŒ–ã€‚

---

**Version**: 2.1.0
**Last Updated**: 2025-11-16
**Status**: Production Ready
**New Capabilities**:
- Pre/Post Hooksè‡ªåŠ¨åŒ–
- æ··åˆå†…å­˜ç³»ç»Ÿ (ChromaDB + SQLite)
- è·¨ä¼šè¯å­¦ä¹ å’Œä¸ªæ€§åŒ–
- è‡ªåŠ¨è´¨é‡ä¿è¯
